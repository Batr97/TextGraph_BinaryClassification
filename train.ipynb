{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset\n",
    "from dgl import AddSelfLoop\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.nn import EdgeGATConv, GraphConv\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CSVDataset('../data/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_datasetloader = dgl.data.utils.split_dataset(dataset, shuffle=True, frac_list=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = GraphDataLoader(dataset, batch_size=32, shuffle=True)\n",
    "train_loader = GraphDataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = GraphDataLoader(val_datasetloader, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs = dgl.unbatch(next(iter(data_loader))[0])\n",
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=9, num_edges=12,\n",
       "      ndata_schemes={'feat': Scheme(shape=(768,), dtype=torch.float32)}\n",
       "      edata_schemes={'feat': Scheme(shape=(768,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))[0]\n",
    "dgl.batch([dataset[0][0], dataset[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 768])\n",
      "torch.Size([164, 768])\n",
      "torch.Size([128, 2, 15])\n",
      "torch.Size([128, 30])\n",
      "torch.Size([128, 2, 15])\n",
      "torch.Size([128, 30])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "g = next(iter(data_loader))[0]\n",
    "\n",
    "# g = dgl.add_self_loop(g)\n",
    "# print(dgl.unbatch(g))\n",
    "print(g.ndata['feat'].shape)\n",
    "layer1 = EdgeGATConv(768, 768, 15, 2, allow_zero_in_degree=True)\n",
    "layer2 = EdgeGATConv(15*2, 768, 15, 2, allow_zero_in_degree=True)\n",
    "# layer1 = GraphConv(768, 15, allow_zero_in_degree=True)\n",
    "# layer2 = GraphConv(15, 2, allow_zero_in_degree=True)\n",
    "lin_layer = nn.Linear(15 * 2, 1)\n",
    "edge_feat = g.edata['feat']\n",
    "node_feat = g.ndata['feat']\n",
    "print(edge_feat.shape)\n",
    "out1 = layer1(g, node_feat, edge_feat, get_attention=False)\n",
    "print(out1.shape)\n",
    "print(out1.view(g.ndata['feat'].shape[0], 30).shape)\n",
    "out2 = F.relu(layer2(g, out1.view(g.ndata['feat'].shape[0], 30), edge_feat))\n",
    "print(out2.shape)\n",
    "out2 = out2.view(g.ndata['feat'].shape[0], 30)\n",
    "print(out2.shape)\n",
    "# with g.local_scope():\n",
    "#     g.ndata['feat'] = out2\n",
    "#     hg1 = dgl.readout_nodes(g, 'feat')\n",
    "#     print(hg1.shape)\n",
    "g.ndata['feat'] = out2\n",
    "hg = dgl.mean_nodes(g, 'feat')\n",
    "print(hg.shape)\n",
    "# he = dgl.mean_edges(g, 'feat')\n",
    "# print(he.shape)\n",
    "out3 = lin_layer(hg)\n",
    "print(out3.shape)\n",
    "\n",
    "# out1 = F.relu(layer1(g, g.ndata['feat']))\n",
    "# print(out1.shape)\n",
    "# out2 = F.relu(layer2(g, out1))\n",
    "# print(out2.shape)\n",
    "# g.ndata['h'] = out2\n",
    "# # dgl.unbatch(g)\n",
    "# dgl.mean_nodes(g, 'h').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_feats, edge_feats, out_feats, num_heads):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.edge_feats = edge_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.num_heads = num_heads\n",
    "        self.conv1 = dglnn.EdgeGATConv(in_feats, edge_feats, out_feats, num_heads, allow_zero_in_degree=True)\n",
    "        # self.conv2 = dglnn.EdgeGATConv(out_feats * num_heads, edge_feats, out_feats, num_heads, allow_zero_in_degree=True)\n",
    "        self.classify = nn.Linear(out_feats * num_heads, 1)\n",
    "\n",
    "    def forward(self, g, node_feat, edge_feat):\n",
    "        h = F.relu(self.conv1(g, node_feat, edge_feat))        \n",
    "        h = h.view(h.shape[0], self.out_feats * self.num_heads)\n",
    "        # h = F.relu(self.conv2(g, h, edge_feat))\n",
    "        # h = h.view(h.shape[0], self.out_feats * self.num_heads)\n",
    "        with g.local_scope():\n",
    "            g.ndata['feat'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = dgl.mean_nodes(g, 'feat')\n",
    "            # he = dgl.mean_edges(g, 'feat')\n",
    "            # hg = torch.cat([hg, he], dim=-1)``\n",
    "            return torch.sigmoid(self.classify(hg))\n",
    "        \n",
    "\n",
    "def f1_metric(logits, labels):\n",
    "    preds = (logits > 0.5).float()\n",
    "    return f1_score(labels.cpu().numpy(), preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/b_gainitdinov/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1760: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "  0%|          | 0/2 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m node_feats \u001b[39m=\u001b[39m batched_graph\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m edge_feats \u001b[39m=\u001b[39m batched_graph\u001b[39m.\u001b[39medata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m logits \u001b[39m=\u001b[39m model(batched_graph, node_feats, edge_feats)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(logits, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb Cell 10\u001b[0m in \u001b[0;36mClassifier.forward\u001b[0;34m(self, g, node_feat, edge_feat)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, node_feat, edge_feat):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(g, node_feat, edge_feat))        \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mview(h\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_feats \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# h = F.relu(self.conv2(g, h, edge_feat))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbanshee/home/b_gainitdinov/por_perm_upscaling/projects/TextGraphs17-shared-task/baselines/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# h = h.view(h.shape[0], self.out_feats * self.num_heads)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mncmu_ganitdinov/lib/python3.9/site-packages/dgl/nn/pytorch/conv/edgegatconv.py:348\u001b[0m, in \u001b[0;36mEdgeGATConv.forward\u001b[0;34m(self, graph, feat, edge_feat, get_attention)\u001b[0m\n\u001b[1;32m    344\u001b[0m graph\u001b[39m.\u001b[39mapply_edges(fn\u001b[39m.\u001b[39mu_add_v(\u001b[39m\"\u001b[39m\u001b[39mel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39me_tmp\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    346\u001b[0m \u001b[39m# e_tmp combines attention weights of source and destination node.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m# Add the attention weight of the edge.\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m graph\u001b[39m.\u001b[39medata[\u001b[39m\"\u001b[39m\u001b[39me\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49medata[\u001b[39m\"\u001b[39;49m\u001b[39me_tmp\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m graph\u001b[39m.\u001b[39;49medata[\u001b[39m\"\u001b[39;49m\u001b[39mee\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    350\u001b[0m \u001b[39m# Create new edges features that combine the\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39m# features of the source node and the edge features.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m graph\u001b[39m.\u001b[39mapply_edges(fn\u001b[39m.\u001b[39mu_add_e(\u001b[39m\"\u001b[39m\u001b[39mft\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mft_edge\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mft_combined\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_feats_dim = 768\n",
    "edge_feats_dim = 768\n",
    "out_feats = 15\n",
    "num_heads = 1\n",
    "\n",
    "model = Classifier(in_feats_dim, edge_feats_dim, out_feats, num_heads)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.BCELoss()\n",
    "# лучшую модельку созхранять, отображать метрику ф1, графики, всякие изменеия лр\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "epoch_losses = []\n",
    "val_losses = []\n",
    "num_epochs = 2\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "    model.train()\n",
    "    for i, (batched_graph, labels) in enumerate(train_loader):\n",
    "        # batched_graph = dgl.add_self_loop(batched_graph)\n",
    "        node_feats = batched_graph.ndata['feat']\n",
    "        edge_feats = batched_graph.edata['feat']\n",
    "        logits = model(batched_graph, node_feats, edge_feats)\n",
    "        loss = loss_func(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        f1 = f1_metric(logits, labels)\n",
    "        epoch_f1 += f1\n",
    "    epoch_loss /= (i + 1)\n",
    "    epoch_f1 /= (i + 1)\n",
    "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for i, (batched_graph, labels) in enumerate(val_loader):\n",
    "            node_feats = batched_graph.ndata['feat']\n",
    "            edge_feats = batched_graph.edata['feat']\n",
    "            logits = model(batched_graph, node_feats, edge_feats)\n",
    "            loss = loss_func(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            f1 = f1_metric(logits, labels)\n",
    "            val_f1 += f1\n",
    "            # Convert logits to predictions\n",
    "            predictions = (logits > 0.5).float()\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_targets.extend(labels.tolist())\n",
    "\n",
    "    val_loss /= (i + 1)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "          f'Train Loss: {epoch_loss:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val F1: {val_f1:.4f}')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict('GAT_model.pth')\n",
    "\n",
    "# After training completes, load the best model state dict\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), 'GAT_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mncmu_ganitdinov': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ec6cb8b45029d7d6f62f4f95c1d6c5e3d549c8394eb72be81c47f2d17ba31b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
